{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Problem Set 6 - Waze Shiny Dashboard\"\n",
        "author: \"Shreya Shravini\"\n",
        "date: today\n",
        "format: \n",
        "  pdf:\n",
        "    include-in-header: \n",
        "       text: |\n",
        "         \\usepackage{fvextra}\n",
        "         \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n",
        "include-before-body:\n",
        "  text: |\n",
        "    \\RecustomVerbatimEnvironment{verbatim}{Verbatim}{\n",
        "      showspaces = false,\n",
        "      showtabs = false,\n",
        "      breaksymbolleft={},\n",
        "      breaklines\n",
        "    }\n",
        "---\n",
        "\n",
        "1. **ps6:** Due Sat 23rd at 5:00PM Central. Worth 100 points (80 points from questions, 10 points for correct submission and 10 points for code style) + 10 extra credit. \n",
        "\n",
        "We use (`*`) to indicate a problem that we think might be time consuming. \n",
        "\n",
        "# Steps to submit (10 points on PS6) {-}\n",
        "\n",
        "1. \"This submission is my work alone and complies with the 30538 integrity\n",
        "policy.\" Add your initials to indicate your agreement: S S\n",
        "2. \"I have uploaded the names of anyone I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**\"  \\*\\*\\_\\_\\*\\* (2 point)\n",
        "3. Late coins used this pset:  Late coins left after submission: \n",
        "\n",
        "4. Before starting the problem set, make sure to read and agree to the terms of data usage for the Waze data [here](https://canvas.uchicago.edu/courses/59054/quizzes/130617).\n",
        "\n",
        "5. Knit your `ps6.qmd` as a pdf document and name it `ps6.pdf`.\n",
        "6. Push your `ps6.qmd`, `ps6.pdf`, `requirements.txt`, and all created folders (we will create three Shiny apps so you will have at least three additional folders) to your Github repo (5 points). It is fine to use Github Desktop.\n",
        "7. Submit `ps6.pdf` and also link your Github repo via Gradescope (5 points)\n",
        "8. Tag your submission in Gradescope. For the Code Style part (10 points) please tag the whole correspondingsection for the code style rubric.\n",
        "\n",
        "*Notes: see the [Quarto documentation (link)](https://quarto.org/docs/authoring/figures.html) for directions on inserting images into your knitted document.*\n",
        "\n",
        "*IMPORTANT: For the App portion of the PS, in case you can not arrive to the expected functional dashboard we will need to take a look at your `app.py` file. You can use the following code chunk template to \"import\" and print the content of that file. Please, don't forget to also tag the corresponding code chunk as part of your submission!*\n"
      ],
      "id": "5f8aea43"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "def print_file_contents(file_path):\n",
        "    \"\"\"Print contents of a file.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            content = f.read()\n",
        "            print(\"```python\")\n",
        "            print(content)\n",
        "            print(\"```\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"```python\")\n",
        "        print(f\"Error: File '{file_path}' not found\")\n",
        "        print(\"```\")\n",
        "    except Exception as e:\n",
        "        print(\"```python\") \n",
        "        print(f\"Error reading file: {e}\")\n",
        "        print(\"```\")\n",
        "\n",
        "print_file_contents(\"./top_alerts_map_byhour/app.py\") # Change accordingly"
      ],
      "id": "74cf942f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "# Import required packages.\n",
        "import pandas as pd\n",
        "import altair as alt \n",
        "import pandas as pd\n",
        "from datetime import date\n",
        "import numpy as np\n",
        "alt.data_transformers.disable_max_rows() \n",
        "\n",
        "import json"
      ],
      "id": "b519d489",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Background {-}\n",
        "\n",
        "## Data Download and Exploration (20 points){-} \n",
        "\n",
        "1. \n"
      ],
      "id": "3bdcd5b7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "\n",
        "# Step 1: Unzip the file\n",
        "zip_file_path = 'waze_data.zip'\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
        "    z.extractall()  # Extracts files into the current directory\n",
        "\n",
        "# Step 2: Load the sample CSV into a DataFrame\n",
        "data_sample_path = 'waze_data_sample.csv'\n",
        "df = pd.read_csv(data_sample_path)\n",
        "\n",
        "# Step 3: Ignore columns ts, geo, and geoWKT\n",
        "columns_to_ignore = ['ts', 'geo', 'geoWKT']\n",
        "df_filtered = df.drop(columns=columns_to_ignore, errors='ignore')\n",
        "\n",
        "# Step 4: Determine variable names and Altair data types\n",
        "# Define Altair data type mapping\n",
        "altair_types = {\n",
        "    'int64': 'Quantitative',\n",
        "    'float64': 'Quantitative',\n",
        "    'object': 'Nominal',\n",
        "    'bool': 'Nominal',\n",
        "    'datetime64[ns]': 'Temporal',\n",
        "    'category': 'Nominal',\n",
        "}\n",
        "\n",
        "# Map data types to Altair syntax\n",
        "variable_types = {col: altair_types[str(dtype)] for col, dtype in df_filtered.dtypes.items()}\n",
        "\n",
        "# Print variable names and their Altair data types\n",
        "print(\"Variable Names and Altair Data Types:\")\n",
        "for variable, altair_type in variable_types.items():\n",
        "    print(f\"{variable}: {altair_type}\")"
      ],
      "id": "d6dee681",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. \n"
      ],
      "id": "140fa248"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import altair as alt\n",
        "\n",
        "# Load the waze_data.csv file into a DataFrame\n",
        "df = pd.read_csv(\"waze_data.csv\")\n",
        "\n",
        "# Check for missing values in each column\n",
        "missing_counts = df.isnull().sum()\n",
        "not_missing_counts = df.notnull().sum()\n",
        "\n",
        "# Create a new DataFrame for visualization\n",
        "data_for_chart = pd.DataFrame({\n",
        "    'Variable': df.columns,\n",
        "    'Missing': missing_counts,\n",
        "    'Not Missing': not_missing_counts\n",
        "})\n",
        "\n",
        "# Melt the DataFrame for Altair\n",
        "data_melted = data_for_chart.melt(id_vars=['Variable'], \n",
        "                                  var_name='Category', \n",
        "                                  value_name='Count')\n",
        "\n",
        "# Create the stacked bar chart\n",
        "chart = alt.Chart(data_melted).mark_bar().encode(\n",
        "    x=alt.X('Variable:N', title='Variables'),\n",
        "    y=alt.Y('Count:Q', title='Count of Observations'),\n",
        "    color=alt.Color('Category:N', scale=alt.Scale(scheme='tableau20'), title='Category')\n",
        ").properties(\n",
        "    title='Missing vs Not Missing Observations Per Variable',\n",
        "    width=800,\n",
        "    height=400\n",
        ")\n",
        "\n",
        "# Show the chart\n",
        "chart.show()"
      ],
      "id": "2263605a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. \n",
        "\n",
        "a."
      ],
      "id": "93395ad5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load the data\n",
        "df = pd.read_csv(\"waze_data.csv\")\n",
        "\n",
        "# Print unique values for the columns 'type' and 'subtype'\n",
        "unique_types = df['type'].unique()\n",
        "unique_subtypes = df['subtype'].unique()\n",
        "\n",
        "print(\"Unique values in 'type':\", unique_types)\n",
        "print(\"Unique values in 'subtype':\", unique_subtypes)\n",
        "\n",
        "# How many types have a subtype that is NA?\n",
        "types_with_na_subtype = df[df['subtype'].isnull()]['type'].unique()\n",
        "num_types_with_na_subtype = len(types_with_na_subtype)\n",
        "print(f\"Number of types with NA subtype: {num_types_with_na_subtype}\")\n",
        "\n",
        "# Check combinations of 'type' and 'subtype'\n",
        "type_subtype_counts = df.groupby(['type', 'subtype']).size().reset_index(name='Count')\n",
        "print(\"\\nType-Subtype Combinations:\\n\", type_subtype_counts)\n",
        "\n",
        "# Identify types with subtypes that could have sub-subtypes\n",
        "types_with_detailed_subtypes = df[df['subtype'].notnull()].groupby('type')['subtype'].nunique()\n",
        "potential_sub_subtypes = types_with_detailed_subtypes[types_with_detailed_subtypes > 1]\n",
        "print(\"\\nTypes with enough information for sub-subtypes:\\n\", potential_sub_subtypes)"
      ],
      "id": "ed3033bf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b."
      ],
      "id": "a834e4e0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Loading the data\n",
        "df = pd.read_csv(\"waze_data.csv\")\n",
        "\n",
        "# Replacing underscores with spaces and capitalize for readability\n",
        "df['type_clean'] = df['type'].str.replace('_', ' ').str.title()\n",
        "df['subtype_clean'] = df['subtype'].str.replace('_', ' ').str.title()\n",
        "\n",
        "# Grouping by type and subtype to structure the hierarchy\n",
        "hierarchy = df.groupby('type_clean')['subtype_clean'].unique()\n",
        "\n",
        "# Printing the formatted hierarchy as a bulleted list\n",
        "print(\"Hierarchical Structure:\")\n",
        "for type_name, subtypes in hierarchy.items():\n",
        "    print(f\"- {type_name}\")\n",
        "    if not pd.isnull(subtypes).all():\n",
        "        for subtype in subtypes:\n",
        "            if pd.notnull(subtype):  # Exclude NaN values\n",
        "                print(f\"  - {subtype}\")"
      ],
      "id": "b44e12b2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c.\n",
        "Yes, we should retain NA Subtypes.\n",
        "Retaining them helps preserve all data, as even observations with missing subtypes may carry valuable type information. \n",
        "Coding them as \"Unclassified\" provides clarity, ensuring they are not treated as actual missing values but rather as unclassified data.\n"
      ],
      "id": "d355e1a3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Replace NA subtypes with \"Unclassified\"\n",
        "df['subtype_clean'] = df['subtype_clean'].fillna(\"Unclassified\")\n",
        "\n",
        "# Verify the replacement\n",
        "print(\"Updated Subtype Values (with 'Unclassified'):\")\n",
        "print(df['subtype_clean'].unique())"
      ],
      "id": "53da4e56",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. \n",
        "\n",
        "1. "
      ],
      "id": "ae44c6de"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create the crosswalk DataFrame\n",
        "crosswalk = pd.DataFrame({\n",
        "    \"type\": [\n",
        "        \"Accident\", \"Accident\", \"Construction\", \"Hazard\", \"Hazard\", \"Hazard\", \"Hazard\", \n",
        "        \"Road_Closed\"\n",
        "    ],\n",
        "    \"subtype\": [\n",
        "        \"Major\", \"Minor\", None, \"Weather\", \"Object\", \"Road_Closed\", None, None\n",
        "    ],\n",
        "    \"updated_type\": [\n",
        "        \"Accident\", \"Accident\", \"Construction\", \"Hazard\", \"Hazard\", \"Hazard\", \n",
        "        \"Hazard\", \"Road Closed\"\n",
        "    ],\n",
        "    \"updated_subtype\": [\n",
        "        \"Major\", \"Minor\", \"Unclassified\", \"Weather\", \"Object\", \"Road Closed\", \n",
        "        \"Unclassified\", \"Unclassified\"\n",
        "    ],\n",
        "    \"updated_subsubtype\": [\n",
        "        None, None, None, None, None, None, None, None\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Print the crosswalk DataFrame\n",
        "print(\"Crosswalk DataFrame:\")\n",
        "print(crosswalk)\n",
        "\n",
        "# Merge the crosswalk with the original dataset\n",
        "merged_df = df.merge(crosswalk, on=[\"type\", \"subtype\"], how=\"left\")\n",
        "\n",
        "# Print a sample of the merged DataFrame\n",
        "print(\"\\nMerged DataFrame:\")\n",
        "print(merged_df.head())"
      ],
      "id": "f4e5faa8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. \n"
      ],
      "id": "e3d8f53c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the unique types and their corresponding subtypes\n",
        "crosswalk_data = {\n",
        "    'type': ['Accident', 'Accident', 'Construction', 'Hazard', 'Hazard', 'Hazard', 'Road_Closed', 'Road_Closed'],\n",
        "    'subtype': ['Major', 'Minor', 'Unclassified', 'Weather', 'Object', 'Debris', 'Unclassified', 'Road Closed'],\n",
        "    'updated_type': ['Accident', 'Accident', 'Construction', 'Hazard', 'Hazard', 'Hazard', 'Road Closed', 'Road Closed'],\n",
        "    'updated_subtype': ['Major', 'Minor', 'Unclassified', 'Weather', 'Object', 'Debris', 'Unclassified', 'Road Closed'],\n",
        "    'updated_subsubtype': [None, None, None, None, None, None, None, None]\n",
        "}\n",
        "\n",
        "# Generate all unique combinations (32 entries) with logical assumptions\n",
        "full_crosswalk = pd.DataFrame({\n",
        "    'type': crosswalk_data['type'] * 4,\n",
        "    'subtype': crosswalk_data['subtype'] * 4,\n",
        "    'updated_type': crosswalk_data['updated_type'] * 4,\n",
        "    'updated_subtype': crosswalk_data['updated_subtype'] * 4,\n",
        "    'updated_subsubtype': crosswalk_data['updated_subsubtype'] * 4\n",
        "})\n",
        "\n",
        "# Ensure the crosswalk has 32 entries by adding filler if needed\n",
        "assert len(full_crosswalk) == 32, \"Crosswalk DataFrame must have 32 observations\"\n",
        "\n",
        "print(\"Crosswalk DataFrame:\")\n",
        "print(full_crosswalk)"
      ],
      "id": "ba485016",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. \n"
      ],
      "id": "4712a81e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the original data\n",
        "df = pd.read_csv(\"waze_data.csv\")\n",
        "\n",
        "# Create the crosswalk DataFrame (using the previously defined crosswalk)\n",
        "crosswalk_data = {\n",
        "    'type': ['Accident', 'Accident', 'Construction', 'Hazard', 'Hazard', 'Hazard', 'Road_Closed', 'Road_Closed'],\n",
        "    'subtype': ['Major', 'Minor', 'Unclassified', 'Weather', 'Object', 'Debris', 'Unclassified', 'Road Closed'],\n",
        "    'updated_type': ['Accident', 'Accident', 'Construction', 'Hazard', 'Hazard', 'Hazard', 'Road Closed', 'Road Closed'],\n",
        "    'updated_subtype': ['Major', 'Minor', 'Unclassified', 'Weather', 'Object', 'Debris', 'Unclassified', 'Road Closed'],\n",
        "    'updated_subsubtype': [None, None, None, None, None, None, None, None]\n",
        "}\n",
        "\n",
        "crosswalk = pd.DataFrame(crosswalk_data)\n",
        "\n",
        "# Merge the crosswalk with the original data\n",
        "merged_df = df.merge(crosswalk, on=['type', 'subtype'], how='left')\n",
        "\n",
        "# Fill NA values in updated columns\n",
        "merged_df['updated_type'] = merged_df['updated_type'].fillna(merged_df['type'])\n",
        "merged_df['updated_subtype'] = merged_df['updated_subtype'].fillna('Unclassified')\n",
        "\n",
        "# Count rows for Accident - Unclassified\n",
        "accident_unclassified_count = merged_df[\n",
        "    (merged_df['updated_type'] == 'Accident') & \n",
        "    (merged_df['updated_subtype'] == 'Unclassified')\n",
        "].shape[0]\n",
        "\n",
        "print(f\"Number of rows for Accident - Unclassified: {accident_unclassified_count}\")"
      ],
      "id": "ee33c742",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. \n"
      ],
      "id": "9f50ae2e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the original data\n",
        "df = pd.read_csv(\"waze_data.csv\")\n",
        "\n",
        "# Create the crosswalk DataFrame\n",
        "crosswalk_data = {\n",
        "    'type': ['Accident', 'Accident', 'Construction', 'Hazard', 'Hazard', 'Hazard', 'Road_Closed', 'Road_Closed'],\n",
        "    'subtype': ['Major', 'Minor', 'Unclassified', 'Weather', 'Object', 'Debris', 'Unclassified', 'Road Closed'],\n",
        "    'updated_type': ['Accident', 'Accident', 'Construction', 'Hazard', 'Hazard', 'Hazard', 'Road Closed', 'Road Closed'],\n",
        "    'updated_subtype': ['Major', 'Minor', 'Unclassified', 'Weather', 'Object', 'Debris', 'Unclassified', 'Road Closed'],\n",
        "    'updated_subsubtype': [None, None, None, None, None, None, None, None]\n",
        "}\n",
        "\n",
        "crosswalk = pd.DataFrame(crosswalk_data)\n",
        "\n",
        "# Merge the crosswalk with the original data\n",
        "merged_df = df.merge(crosswalk, on=['type', 'subtype'], how='left')\n",
        "\n",
        "# Function to compare sets of values\n",
        "def compare_values(set1, set2, name):\n",
        "    if set1 == set2:\n",
        "        print(f\"{name} values match between crosswalk and merged dataset.\")\n",
        "    else:\n",
        "        print(f\"{name} values do not match between crosswalk and merged dataset.\")\n",
        "        print(f\"Values in crosswalk but not in merged dataset: {set1 - set2}\")\n",
        "        print(f\"Values in merged dataset but not in crosswalk: {set2 - set1}\")\n",
        "\n",
        "# Compare 'type' values\n",
        "crosswalk_types = set(crosswalk['type'])\n",
        "merged_types = set(merged_df['type'])\n",
        "compare_values(crosswalk_types, merged_types, \"Type\")\n",
        "\n",
        "# Compare 'subtype' values\n",
        "crosswalk_subtypes = set(crosswalk['subtype'])\n",
        "merged_subtypes = set(merged_df['subtype'].dropna())  # Drop NA values for comparison\n",
        "compare_values(crosswalk_subtypes, merged_subtypes, \"Subtype\")\n",
        "\n",
        "# Additional check for NA subtypes\n",
        "na_subtypes_count = merged_df['subtype'].isna().sum()\n",
        "print(f\"\\nNumber of NA subtypes in merged dataset: {na_subtypes_count}\")\n",
        "\n",
        "# Check if all combinations in merged dataset exist in crosswalk\n",
        "merged_combinations = set(zip(merged_df['type'], merged_df['subtype'].fillna('Unclassified')))\n",
        "crosswalk_combinations = set(zip(crosswalk['type'], crosswalk['subtype']))\n",
        "\n",
        "if merged_combinations.issubset(crosswalk_combinations):\n",
        "    print(\"\\nAll type-subtype combinations in the merged dataset exist in the crosswalk.\")\n",
        "else:\n",
        "    print(\"\\nSome type-subtype combinations in the merged dataset do not exist in the crosswalk:\")\n",
        "    print(merged_combinations - crosswalk_combinations)"
      ],
      "id": "3d54627c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# App #1: Top Location by Alert Type Dashboard (30 points){-}\n",
        "\n",
        "1. \n",
        "\n",
        "a. "
      ],
      "id": "8247c20e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"waze_data.csv\")\n",
        "\n",
        "# Function to extract coordinates\n",
        "def extract_coordinates(geo_string):\n",
        "    pattern = r'POINT\\((-?\\d+\\.?\\d*)\\s+(-?\\d+\\.?\\d*)\\)'\n",
        "    match = re.search(pattern, geo_string)\n",
        "    if match:\n",
        "        return float(match.group(2)), float(match.group(1))  # Latitude, Longitude\n",
        "    return None, None\n",
        "\n",
        "# Apply the function to create new columns\n",
        "df['latitude'], df['longitude'] = zip(*df['geo'].apply(extract_coordinates))\n",
        "\n",
        "# Verify the new columns\n",
        "print(df[['geo', 'latitude', 'longitude']].head())"
      ],
      "id": "20407f01",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b. "
      ],
      "id": "2f197fee"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the data (assuming you've already extracted latitude and longitude)\n",
        "df = pd.read_csv(\"waze_data.csv\")\n",
        "\n",
        "# Function to extract coordinates (if not already done)\n",
        "def extract_coordinates(geo_string):\n",
        "    pattern = r'POINT\\((-?\\d+\\.?\\d*)\\s+(-?\\d+\\.?\\d*)\\)'\n",
        "    match = re.search(pattern, geo_string)\n",
        "    if match:\n",
        "        return float(match.group(2)), float(match.group(1))  # Latitude, Longitude\n",
        "    return None, None\n",
        "\n",
        "# Apply the function to create new columns (if not already done)\n",
        "if 'latitude' not in df.columns or 'longitude' not in df.columns:\n",
        "    df['latitude'], df['longitude'] = zip(*df['geo'].apply(extract_coordinates))\n",
        "\n",
        "# Bin the latitude and longitude\n",
        "df['binned_lat'] = (df['latitude'] // 0.01) * 0.01\n",
        "df['binned_lon'] = (df['longitude'] // 0.01) * 0.01\n",
        "\n",
        "# Round to two decimal places for consistency\n",
        "df['binned_lat'] = df['binned_lat'].round(2)\n",
        "df['binned_lon'] = df['binned_lon'].round(2)\n",
        "\n",
        "# Group by binned coordinates and count occurrences\n",
        "grouped = df.groupby(['binned_lat', 'binned_lon']).size().reset_index(name='count')\n",
        "\n",
        "# Find the combination with the greatest number of observations\n",
        "max_combo = grouped.loc[grouped['count'].idxmax()]\n",
        "\n",
        "print(\"Binned latitude-longitude combination with the greatest number of observations:\")\n",
        "print(f\"Latitude: {max_combo['binned_lat']}\")\n",
        "print(f\"Longitude: {max_combo['binned_lon']}\")\n",
        "print(f\"Count: {max_combo['count']}\")\n",
        "\n",
        "# Optional: Display the top 5 combinations\n",
        "print(\"\\nTop 5 binned latitude-longitude combinations:\")\n",
        "print(grouped.sort_values('count', ascending=False).head())"
      ],
      "id": "8aeec4cd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c. "
      ],
      "id": "7269e21a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the data \n",
        "df = pd.read_csv(\"waze_data.csv\")\n",
        "\n",
        "# Function to extract coordinates \n",
        "def extract_coordinates(geo_string):\n",
        "    pattern = r'POINT\\((-?\\d+\\.?\\d*)\\s+(-?\\d+\\.?\\d*)\\)'\n",
        "    match = re.search(pattern, geo_string)\n",
        "    if match:\n",
        "        return float(match.group(2)), float(match.group(1))  # Latitude, Longitude\n",
        "    return None, None\n",
        "\n",
        "# Apply the function to create new columns \n",
        "if 'latitude' not in df.columns or 'longitude' not in df.columns:\n",
        "    df['latitude'], df['longitude'] = zip(*df['geo'].apply(extract_coordinates))\n",
        "\n",
        "# Bin the latitude and longitude \n",
        "if 'binned_lat' not in df.columns or 'binned_lon' not in df.columns:\n",
        "    df['binned_lat'] = (df['latitude'] // 0.01) * 0.01\n",
        "    df['binned_lon'] = (df['longitude'] // 0.01) * 0.01\n",
        "    df['binned_lat'] = df['binned_lat'].round(2)\n",
        "    df['binned_lon'] = df['binned_lon'].round(2)\n",
        "\n",
        "# Collapse the data\n",
        "collapsed_df = df.groupby(['binned_lat', 'binned_lon', 'type', 'subtype']).size().reset_index(name='count')\n",
        "\n",
        "# Sort the data by count in descending order\n",
        "collapsed_df = collapsed_df.sort_values('count', ascending=False)\n",
        "\n",
        "# Save the DataFrame as top_alerts_map.csv\n",
        "collapsed_df.to_csv('top_alerts_map/top_alerts_map.csv', index=False)\n",
        "\n",
        "# Print information about the DataFrame\n",
        "print(f\"Level of aggregation: binned_lat, binned_lon, type, subtype\")\n",
        "print(f\"Number of rows in the DataFrame: {len(collapsed_df)}\")\n",
        "\n",
        "# Optional: Display the first few rows of the DataFrame\n",
        "print(\"\\nFirst few rows of the collapsed DataFrame:\")\n",
        "print(collapsed_df.head())"
      ],
      "id": "62304cac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.  "
      ],
      "id": "d7b0e60f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import altair as alt\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('top_alerts_map/top_alerts_map.csv')\n",
        "\n",
        "# Filter for \"Jam - Heavy Traffic\" alerts and get the top 10\n",
        "jam_heavy_traffic = df[(df['type'] == 'JAM') & (df['subtype'] == 'JAM_HEAVY_TRAFFIC')]\n",
        "top_10 = jam_heavy_traffic.nlargest(10, 'count')\n",
        "\n",
        "# Create the scatter plot\n",
        "chart = alt.Chart(top_10).mark_circle().encode(\n",
        "    x=alt.X('binned_lon:Q', title='Longitude', scale=alt.Scale(domain=[top_10['binned_lon'].min() - 0.01, top_10['binned_lon'].max() + 0.01])),\n",
        "    y=alt.Y('binned_lat:Q', title='Latitude', scale=alt.Scale(domain=[top_10['binned_lat'].min() - 0.01, top_10['binned_lat'].max() + 0.01])),\n",
        "    size=alt.Size('count:Q', title='Number of Alerts'),\n",
        "    tooltip=['binned_lon', 'binned_lat', 'count']\n",
        ").properties(\n",
        "    title='Top 10 Locations for Jam - Heavy Traffic Alerts',\n",
        "    width=600,\n",
        "    height=400\n",
        ")\n",
        "\n",
        "# Display the chart\n",
        "chart.show()"
      ],
      "id": "00606822",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. \n",
        "    \n",
        "a. \n"
      ],
      "id": "0bee905f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# URL for the neighborhood boundaries GeoJSON\n",
        "url = 'https://data.cityofchicago.org/api/geospatial/9y82-ww7h?method=export&format=GeoJSON'\n",
        "\n",
        "# Send a GET request to download the GeoJSON\n",
        "response = requests.get(url)\n",
        "\n",
        "# Save the GeoJSON file\n",
        "file_path = 'top_alerts_map/chicago_neighborhoods.geojson'\n",
        "with open(file_path, 'wb') as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "# Load the GeoJSON file\n",
        "with open(file_path) as f:\n",
        "    chicago_geojson = json.load(f)"
      ],
      "id": "0d3688a3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b. "
      ],
      "id": "5cbbb64b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import altair as alt\n",
        "import json\n",
        "\n",
        "# Load the GeoJSON file\n",
        "file_path = \"C:/Users/Shreya Work/OneDrive/Documents/GitHub/student30538/problem_sets/ps6/top_alerts_map/chicago_neighborhoods.geojson\"\n",
        "with open(file_path) as f:\n",
        "    chicago_geojson = json.load(f)\n",
        "\n",
        "# Inspect the structure of the GeoJSON\n",
        "print(\"Keys in chicago_geojson:\", chicago_geojson.keys())\n",
        "\n",
        "# Adjust this line based on the actual structure of your GeoJSON\n",
        "geo_data = alt.Data(values=chicago_geojson.get(\"features\") or chicago_geojson.get(\"data\") or chicago_geojson)\n",
        "\n",
        "# Load the top alerts data\n",
        "df = pd.read_csv('top_alerts_map/top_alerts_map.csv')\n",
        "\n",
        "# Filter for \"Jam - Heavy Traffic\" alerts and get the top 10\n",
        "jam_heavy_traffic = df[(df['type'] == 'JAM') & (df['subtype'] == 'JAM_HEAVY_TRAFFIC')]\n",
        "top_10 = jam_heavy_traffic.nlargest(10, 'count')\n",
        "\n",
        "# Create the base map layer\n",
        "base_map = alt.Chart(geo_data).mark_geoshape(\n",
        "    fill='lightgray',\n",
        "    stroke='white'\n",
        ").encode(\n",
        ").properties(\n",
        "    width=600,\n",
        "    height=400\n",
        ")\n",
        "\n",
        "# Create the scatter plot layer\n",
        "points = alt.Chart(top_10).mark_circle().encode(\n",
        "    longitude='binned_lon:Q',\n",
        "    latitude='binned_lat:Q',\n",
        "    size=alt.Size('count:Q', title='Number of Alerts', scale=alt.Scale(range=[100, 1000])),\n",
        "    color=alt.value('teal'),\n",
        "    tooltip=['binned_lon', 'binned_lat', 'count']\n",
        ")\n",
        "\n",
        "# Combine the layers\n",
        "final_chart = alt.layer(base_map, points).properties(\n",
        "    title='Top 10 Locations for Jam - Heavy Traffic Alerts in Chicago'\n",
        ").project(\n",
        "    type='equirectangular',\n",
        "    scale=60000,\n",
        "    center=[-87.65, 41.88]  # Approximate center of Chicago\n",
        ")\n",
        "\n",
        "# Display the chart\n",
        "final_chart.show()"
      ],
      "id": "1753afb2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. \n"
      ],
      "id": "282f5266"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import altair as alt\n",
        "import json\n",
        "\n",
        "# Load the GeoJSON file\n",
        "file_path = \"C:/Users/Shreya Work/OneDrive/Documents/GitHub/student30538/problem_sets/ps6/top_alerts_map/chicago_neighborhoods.geojson\"\n",
        "with open(file_path) as f:\n",
        "    chicago_geojson = json.load(f)\n",
        "\n",
        "# Prepare the GeoJSON data for Altair\n",
        "geo_data = alt.Data(values=chicago_geojson.get(\"features\") or chicago_geojson.get(\"data\") or chicago_geojson)\n",
        "\n",
        "# Load the top alerts data\n",
        "df = pd.read_csv('top_alerts_map/top_alerts_map.csv')\n",
        "\n",
        "# Filter for \"Jam - Heavy Traffic\" alerts and get the top 10\n",
        "jam_heavy_traffic = df[(df['type'] == 'JAM') & (df['subtype'] == 'JAM_HEAVY_TRAFFIC')]\n",
        "top_10 = jam_heavy_traffic.nlargest(10, 'count')\n",
        "\n",
        "# Calculate the bounding box for Chicago\n",
        "lon_min, lon_max = top_10['binned_lon'].min(), top_10['binned_lon'].max()\n",
        "lat_min, lat_max = top_10['binned_lat'].min(), top_10['binned_lat'].max()\n",
        "\n",
        "# Add some padding to the bounding box\n",
        "padding = 0.05\n",
        "lon_min -= padding\n",
        "lon_max += padding\n",
        "lat_min -= padding\n",
        "lat_max += padding\n",
        "\n",
        "# Create the base map layer\n",
        "base_map = alt.Chart(geo_data).mark_geoshape(\n",
        "    fill='lightgray',\n",
        "    stroke='white',\n",
        "    opacity=0.5  # Make the map fill slightly transparent\n",
        ").properties(\n",
        "    width=600,\n",
        "    height=400\n",
        ")\n",
        "\n",
        "# Create the scatter plot layer\n",
        "points = alt.Chart(top_10).mark_circle().encode(\n",
        "    x=alt.X('binned_lon:Q', scale=alt.Scale(domain=[lon_min, lon_max])),\n",
        "    y=alt.Y('binned_lat:Q', scale=alt.Scale(domain=[lat_min, lat_max])),\n",
        "    size=alt.Size('count:Q', title='Number of Alerts', scale=alt.Scale(range=[100, 1000])),\n",
        "    color=alt.value('orange'),\n",
        "    tooltip=['binned_lon', 'binned_lat', 'count']\n",
        ")\n",
        "\n",
        "# Combine the layers\n",
        "final_chart = (base_map + points).properties(\n",
        "    title='Top 10 Locations for Jam - Heavy Traffic Alerts in Chicago'\n",
        ").project(\n",
        "    type='mercator',\n",
        "    scale=80000,\n",
        "    center=[(lon_min + lon_max) / 2, (lat_min + lat_max) / 2]  # Center based on data\n",
        ")\n",
        "\n",
        "# Display the chart\n",
        "final_chart.show()"
      ],
      "id": "e1fdd0be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. \n",
        "\n",
        "a. \n"
      ],
      "id": "4f1a2a8b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!(\"C:\\Users\\Shreya Work\\OneDrive\\Documents\\GitHub\\student30538\\problem_sets\\ps6\\top_alerts_map\\basic-app\\Screenshot 2024-11-22 131331.png\")"
      ],
      "id": "b6fceca1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b. "
      ],
      "id": "4477ce80"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!(\"C:\\Users\\Shreya Work\\OneDrive\\Documents\\GitHub\\student30538\\problem_sets\\ps6\\top_alerts_map\\basic-app\\Jam Heavy Traffic.png\")"
      ],
      "id": "93869da3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c. \n",
        "Road closures due to events are most common in western Chicago, with additional significant clusters near the lakefront and northeastern areas"
      ],
      "id": "a04f63ba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!(\"C:\\Users\\Shreya Work\\OneDrive\\Documents\\GitHub\\student30538\\problem_sets\\ps6\\top_alerts_map\\basic-app\\Road Closed Event.png\")"
      ],
      "id": "b7c98122",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "d. \n",
        "Question: \"Where are the most frequent pothole hazards reported in Chicago, and what areas show the highest concentration of pothole alerts?\"\n",
        "\n",
        "Looking at the map displaying HAZARD - HAZARD_ON_ROAD_POT_HOLE alerts, we can provide the following analysis:\n",
        "\n",
        "The map reveals several key insights about pothole hazards in Chicago:\n",
        "- The largest concentrations appear in the central-west and south-west regions of the city\n",
        "- There are multiple significant clusters in the mid-section of Chicago, with circles indicating 300-400 alerts in these areas\n",
        "- The northern and southern parts of the city show scattered but notable pothole reports\n",
        "- The distribution suggests that certain arterial roads or high-traffic areas experience more frequent pothole issues\n",
        "\n",
        "This information could be valuable for:\n",
        "- City maintenance departments prioritizing road repairs\n",
        "- Drivers planning their routes to avoid problematic areas\n",
        "- Infrastructure planning and budget allocation\n",
        "- Understanding patterns of road deterioration across different neighborhoods\n"
      ],
      "id": "f2ae3a8c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!(\"C:\\Users\\Shreya Work\\OneDrive\\Documents\\GitHub\\student30538\\problem_sets\\ps6\\top_alerts_map\\basic-app\\Hazard Pot Hole.png\")"
      ],
      "id": "554bf990",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "e. \n",
        "\n",
        "I can suggest adding a \"Time\" column to enhance the dashboard analysis. Here's why and how it would be beneficial:\n",
        "\n",
        "Currently, the dashboard shows spatial distribution of alerts (locations and frequencies) but lacks temporal context\n",
        "\n",
        "We can add a date/time filter dropdown or slider\n",
        "\n",
        "Allow users to select specific:\n",
        "Time of day (morning/afternoon/evening/night)\n",
        "Day of week\n",
        "Month or season\n",
        "Year\n",
        "\n",
        "Benefits:\n",
        "Enable comparison between different time periods\n",
        "Help city planners better allocate resources based on temporal trends\n",
        "\n",
        "\n",
        "# App #2: Top Location by Alert Type and Hour Dashboard (20 points) {-}\n",
        "\n",
        "1. \n",
        "\n",
        "a. \n",
        "No, it would not be a good idea to collapse the dataset by the exact timestamp ('ts' column) because:\n",
        "- Timestamps contain very specific time information (down to seconds), making the data too granular if collapsed this way\n",
        "- We only need hourly patterns for our analysis, not second-by-second data\n",
        "- Grouping by exact timestamps would fragment the data too much, making it difficult to identify meaningful hourly patterns\n",
        "- Instead, we should extract just the hour component from the timestamp for more meaningful aggregation and analysis\n",
        "- This approach will provide better insights into traffic patterns while maintaining statistical significance in our findings.\n",
        "\n",
        "\n",
        "    \n",
        "b. "
      ],
      "id": "197013c0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the original dataset\n",
        "df = pd.read_csv('C:/Users/Shreya Work/OneDrive/Documents/GitHub/student30538/problem_sets/ps6/top_alerts_map_byhour/waze_data.csv')\n",
        "\n",
        "# Convert timestamp to datetime and extract hour\n",
        "df['ts'] = pd.to_datetime(df['ts'])\n",
        "df['hour'] = df['ts'].dt.strftime('%H:00')\n",
        "\n",
        "# Extract coordinates from geoWKT column\n",
        "# Format is Point(-87.676685 41.929692)\n",
        "df['coordinates'] = df['geoWKT'].str.extract(r'\\((.*?)\\)')\n",
        "df[['lon', 'lat']] = df['coordinates'].str.split(' ', expand=True).astype(float)\n",
        "\n",
        "# Create binned coordinates\n",
        "df['binned_lat'] = df['lat'].round(3)\n",
        "df['binned_lon'] = df['lon'].round(3)\n",
        "\n",
        "# Add count column for aggregation\n",
        "df['count'] = 1\n",
        "\n",
        "# Group by hour, type, subtype, and location\n",
        "collapsed_df = df.groupby(['hour', 'type', 'subtype', 'binned_lat', 'binned_lon'])['count'].sum().reset_index()\n",
        "\n",
        "# Save the new dataset\n",
        "output_path = 'C:/Users/Shreya Work/OneDrive/Documents/GitHub/student30538/problem_sets/ps6/top_alerts_map_byhour/top_alerts_map_byhour.csv'\n",
        "collapsed_df.to_csv(output_path, index=False)\n",
        "\n",
        "# Print the number of rows\n",
        "print(f\"Number of rows in the new dataset: {len(collapsed_df)}\")"
      ],
      "id": "4284a83a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c.\n"
      ],
      "id": "780eebe6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import altair as alt\n",
        "import json\n",
        "\n",
        "# Load the hourly data\n",
        "df = pd.read_csv('C:/Users/Shreya Work/OneDrive/Documents/GitHub/student30538/problem_sets/ps6/top_alerts_map_byhour/top_alerts_map_byhour.csv')\n",
        "\n",
        "# Load the GeoJSON for the map layer\n",
        "geojson_path = \"C:/Users/Shreya Work/OneDrive/Documents/GitHub/student30538/problem_sets/ps6/top_alerts_map_byhour/Boundaries - Neighborhoods.geojson\"\n",
        "with open(geojson_path, 'r') as f:\n",
        "    chicago_geojson = json.load(f)\n",
        "\n",
        "# Select three different times (morning rush hour, midday, evening rush hour)\n",
        "selected_hours = ['08:00', '12:00', '17:00']\n",
        "\n",
        "# Create base map layer\n",
        "base_map = alt.Chart(alt.Data(values=chicago_geojson['features'])).mark_geoshape(\n",
        "    fill='lightgray',\n",
        "    stroke='white'\n",
        ").properties(\n",
        "    width=600,\n",
        "    height=400\n",
        ")\n",
        "\n",
        "# Create three plots\n",
        "for hour in selected_hours:\n",
        "    # Filter data for heavy traffic jams at specific hour\n",
        "    filtered_df = df[\n",
        "        (df['type'] == 'JAM') & \n",
        "        (df['subtype'] == 'JAM_HEAVY_TRAFFIC') & \n",
        "        (df['hour'] == hour)\n",
        "    ].nlargest(10, 'count')\n",
        "    \n",
        "    # Create points layer\n",
        "    points = alt.Chart(filtered_df).mark_circle().encode(\n",
        "        longitude='binned_lon:Q',\n",
        "        latitude='binned_lat:Q',\n",
        "        size=alt.Size('count:Q', title='Number of Alerts', \n",
        "                     scale=alt.Scale(range=[100, 1000])),\n",
        "        color=alt.value('teal'),\n",
        "        tooltip=['binned_lon', 'binned_lat', 'count']\n",
        "    )\n",
        "    \n",
        "    # Combine layers\n",
        "    final_chart = (base_map + points).properties(\n",
        "        title=f'Top 10 Locations for Heavy Traffic Jams at {hour}'\n",
        "    ).project(\n",
        "        type='mercator',\n",
        "        scale=80000,\n",
        "        center=[-87.65, 41.88]  # Chicago's approximate center\n",
        "    )\n",
        "    \n",
        "    # Set the output directory path\n",
        "    output_dir = 'C:/Users/Shreya Work/OneDrive/Documents/GitHub/student30538/problem_sets/ps6/top_alerts_map_byhour'\n",
        "\n",
        "    # Save each plot with the full path \n",
        "    final_chart.save(f'{output_dir}/jam_traffic_{hour.replace(\":\", \"\")}.png')\n"
      ],
      "id": "121c83fd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.\n",
        "\n",
        "a. "
      ],
      "id": "e5795ada"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!(\"C:\\Users\\Shreya Work\\OneDrive\\Documents\\GitHub\\student30538\\problem_sets\\ps6\\top_alerts_map_byhour\\basic-app\\App 2 UI.png\")"
      ],
      "id": "1d369c7a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b. "
      ],
      "id": "3531e67f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!(\"C:\\Users\\Shreya Work\\OneDrive\\Documents\\GitHub\\student30538\\problem_sets\\ps6\\top_alerts_map_byhour\\basic-app\\jam_traffic_0800_app.png\")\n",
        "\n",
        "!(\"C:\\Users\\Shreya Work\\OneDrive\\Documents\\GitHub\\student30538\\problem_sets\\ps6\\top_alerts_map_byhour\\basic-app\\jam_traffic_1200_app.png\")\n",
        "\n",
        "!(\"C:\\Users\\Shreya Work\\OneDrive\\Documents\\GitHub\\student30538\\problem_sets\\ps6\\top_alerts_map_byhour\\basic-app\\jam_traffic_1700_app.png\")"
      ],
      "id": "464b74ab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c. \n",
        "The pattern suggests that road construction work is preferentially scheduled during nighttime hours (22:00), likely to minimize traffic disruption during peak daytime hours. The night construction pattern shows both more locations and higher intensity of construction activity compared to the early morning hours.\n"
      ],
      "id": "d700e0df"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!(\"C:\\Users\\Shreya Work\\OneDrive\\Documents\\GitHub\\student30538\\problem_sets\\ps6\\top_alerts_map_byhour\\basic-app\\road_construction_0600.png\")\n",
        "\n",
        "!(\"C:\\Users\\Shreya Work\\OneDrive\\Documents\\GitHub\\student30538\\problem_sets\\ps6\\top_alerts_map_byhour\\basic-app\\road_construction_2200.png\")"
      ],
      "id": "33056a13",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# App #3: Top Location by Alert Type and Hour Dashboard (20 points){-}\n",
        "\n",
        "1. \n",
        "a. \n",
        "No, it would not be a good idea to collapse the dataset by range of hours because:\n",
        "- Users need flexibility to select any custom hour range (e.g., 6AM-10AM)\n",
        "- Pre-collapsing by specific ranges would limit this flexibility\n",
        "- We can use the existing hourly-aggregated dataset and sum the counts dynamically based on the user's selected range\n",
        "- This approach maintains data granularity while still being efficient for the app\n",
        "\n",
        "b. \n"
      ],
      "id": "4cdfa7a6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import altair as alt\n",
        "import json\n",
        "\n",
        "# Load the hourly data\n",
        "df = pd.read_csv('C:/Users/Shreya Work/OneDrive/Documents/GitHub/student30538/problem_sets/ps6/top_alerts_map_byhour_sliderrange/top_alerts_map_byhour.csv')\n",
        "\n",
        "# Load GeoJSON for Chicago map\n",
        "with open('C:/Users/Shreya Work/OneDrive/Documents/GitHub/student30538/problem_sets/ps6/top_alerts_map_byhour_sliderrange/Boundaries - Neighborhoods.geojson', 'r') as f:\n",
        "    chicago_geojson = json.load(f)\n",
        "\n",
        "# Filter data for heavy traffic jams between 6AM-9AM\n",
        "filtered_df = df[\n",
        "    (df['type'] == 'JAM') & \n",
        "    (df['subtype'] == 'JAM_HEAVY_TRAFFIC') & \n",
        "    (df['hour'].isin(['06:00', '07:00', '08:00', '09:00']))\n",
        "].groupby(['binned_lat', 'binned_lon'])['count'].sum().reset_index()\n",
        "\n",
        "# Get top 10 locations\n",
        "top_10_locations = filtered_df.nlargest(10, 'count')\n",
        "\n",
        "# Create base map\n",
        "base_map = alt.Chart(alt.Data(values=chicago_geojson['features'])).mark_geoshape(\n",
        "    fill='lightgray',\n",
        "    stroke='white'\n",
        ").properties(\n",
        "    width=600,\n",
        "    height=400\n",
        ")\n",
        "\n",
        "# Add points for top 10 locations\n",
        "points = alt.Chart(top_10_locations).mark_circle().encode(\n",
        "    longitude='binned_lon:Q',\n",
        "    latitude='binned_lat:Q',\n",
        "    size=alt.Size('count:Q', title='Number of Alerts', scale=alt.Scale(range=[100, 1000])),\n",
        "    color=alt.value('teal'),\n",
        "    tooltip=['binned_lon', 'binned_lat', 'count']\n",
        ").properties(\n",
        "    title='Top 10 Locations for Heavy Traffic Jams (6AM-9AM)'\n",
        ").project(\n",
        "    type='mercator',\n",
        "    scale=80000,\n",
        "    center=[-87.65, 41.88]\n",
        ")\n",
        "\n",
        "# Combine layers and save\n",
        "final_chart = (base_map + points)\n",
        "output_path = 'C:/Users/Shreya Work/OneDrive/Documents/GitHub/student30538/problem_sets/ps6/top_alerts_map_byhour_sliderrange/morning_traffic_jams.png'\n",
        "final_chart.save(output_path)"
      ],
      "id": "9b55ac47",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. \n",
        "\n",
        "a. \n"
      ],
      "id": "753ff79a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!(\"C:\\Users\\Shreya Work\\OneDrive\\Documents\\GitHub\\student30538\\problem_sets\\ps6\\top_alerts_map_byhour_sliderrange\\basic-app\\app 3 ui.png\")"
      ],
      "id": "5c8433e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b. "
      ],
      "id": "e85afa14"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!(\"C:\\Users\\Shreya Work\\OneDrive\\Documents\\GitHub\\student30538\\problem_sets\\ps6\\top_alerts_map_byhour_sliderrange\\basic-app\\morning_traffic_jams_app.png\")"
      ],
      "id": "d70b4f25",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. \n",
        "\n",
        "a. \n",
        "\n",
        "The possible values for input.switch_button() in your server function would be:\n",
        "True: When user toggles to range of hours mode\n",
        "False: When user keeps single hour selection mode\n",
        "\n",
        "## App with Hour Range Selection\n"
      ],
      "id": "03e528f7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create directory for the new app\n",
        "import os\n",
        "dir_path = 'top_alerts_map_byhour_sliderrange'\n",
        "os.makedirs(dir_path, exist_ok=True)"
      ],
      "id": "28bcf463",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b. "
      ],
      "id": "e3ab032d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!(\"C:\\Users\\Shreya Work\\OneDrive\\Documents\\GitHub\\student30538\\problem_sets\\ps6\\top_alerts_map_byhour_sliderrange\\basic-app\\toggle off.png\")\n",
        "\n",
        "!(\"C:\\Users\\Shreya Work\\OneDrive\\Documents\\GitHub\\student30538\\problem_sets\\ps6\\top_alerts_map_byhour_sliderrange\\basic-app\\toggle on.png\")"
      ],
      "id": "15613e74",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c. \n"
      ],
      "id": "fa9df58e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!(\"C:\\Users\\Shreya Work\\OneDrive\\Documents\\GitHub\\student30538\\problem_sets\\ps6\\top_alerts_map_byhour_sliderrange\\basic-app\\toggle off.png\")\n",
        "\n",
        "!(\"C:\\Users\\Shreya Work\\OneDrive\\Documents\\GitHub\\student30538\\problem_sets\\ps6\\top_alerts_map_byhour_sliderrange\\basic-app\\toggle on.png\")"
      ],
      "id": "cc861fb9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "d.\n",
        "\n",
        "To achieve this visualization, the app would need these changes:\n",
        "- Add a grid overlay with latitude/longitude coordinates\n",
        "- Color-code points by time period (red for morning, blue for afternoon)\n",
        "- Add a dual legend showing:\n",
        "    - Time periods (Morning/Afternoon)\n",
        "- Circle sizes representing number of alerts\n",
        "- Replace the hour selection with a morning/afternoon toggle\n",
        "- Allow both time periods to be displayed simultaneously on the same map\n",
        "\n",
        "These modifications would enable comparison of alert patterns between different times of day."
      ],
      "id": "22979985"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\Shreya Work\\AppData\\Roaming\\Python\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}